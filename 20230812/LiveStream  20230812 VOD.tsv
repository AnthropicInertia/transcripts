start	end	text
0	10000	What topic? I am going to try to take this out.
10000	12000	Do you have any topic in mind?
12000	22000	I have. Before that, let me start with a brief introduction and the reason why I am choosing to stream.
23000	31000	I don't have much of a viewer and this is not being streamed on Twitch. This is purely being streamed to YouTube for the vault purpose.
31000	42000	It's almost like a time capsule. That purpose. Probably not relevant for next 2 years, 3 years, 5 years.
42000	49000	But maybe 50 years down the line, somebody might find this and it might add some context, some meaning.
49000	52000	I don't know. Hopefully, that is the case.
59000	64000	I feel that we have been having this chat. We have been having the conversation for the last couple of weeks.
64000	72000	There has been a lot of interesting topics that we kind of touched. But the volatility of it, I kind of don't enjoy that part.
72000	85000	Even though I see the trying to be that Ozymandias and also see the irony of it, but I also can't, as a life form, I cannot just go into the dark night.
85000	92000	I have to fight back against it. So that's kind of the way to try to retain as much information as possible.
92000	100000	Hopefully, that is relevant or useful for us or to somebody else for that matter at any point in time.
100000	110000	With that said, for anybody watching this, I'm Deb. This is Kaustubh. We go a long way. We have had previous ventures.
110000	123000	So we have a tendency of imagining a future and then talking about it and then getting excited about it and then doing something and then realizing that the world hasn't caught up to it yet and it needs more time.
123000	129000	So by the time the world actually catches up with it, we kind of are done with it, exhausted about it and bored about it.
129000	138000	And we moved on to something different that had happened with the entire three pillars of the things that we are working with right now.
138000	152000	The world has caught up to the part about unified communication, self-autonomous software, and eventually going to be that he brain machine interface.
152000	161000	Those are the things that we are kind of in that zone right now, which is good. Applet died for that scene 10 years back.
161000	173000	Anyway, so hopefully the things that we're talking about now, it becomes relevant at some point. Topics that I have been trying to jot them down in notion.
173000	182000	I kind of have like it's not very sorted or organized, but I think I do better thinking when I'm trying to vocalize it and trying to find the pitfalls.
182000	191000	So that is kind of my outlet, like discussing with you and figuring out where I'm coming wrong or where are the parts that are darker still that I need to shed more light on.
191000	202000	And I think you also have some some things to discuss. So tossing the mic back on to you. Go ahead. Where do you want to go?
202000	214000	So, I mean, content warning for viewers more on the lines that you this is more like choosing to record our thoughts.
214000	228000	And it tends to be sometimes utterly specific, thus being very narrow and sometimes extremely generalistic, thus being not applicable to specific context.
228000	239000	The pedal is getting raw information as it processes and comes out of the brain.
239000	248000	The benefit that I hope that even I wish to have when I look back at this in the future is to see the thoughts forming.
249000	256000	And sometimes, you know, when you look back at the process of thought, it's kind of like an RPG.
256000	269000	Like if you pause and play back and wonder if I had thought something different here, kind of like tweaking the variable of my own thought, would that produce a different outcome and then just run tests?
269000	280000	So I would say one of the purpose for me even regarding it at this stage is probably the like wanting the ability to actually come back to it, analyze it.
280000	293000	So maybe I think this will become a topic in a lot of our discussions, the retrievability of information and what does it need to maximize that?
293000	303000	And I guess that is sort of one of the segues to some of the topics that we have been discussing.
303000	312000	Let's say I'd like to start with a few of my old Bangla shots.
312000	324000	Like an elder brother of mine had explained to me when I was about four or five years of age, I'm just learning Bangla of the types of students that exist in a school.
324000	339000	And he had said, and the equivalents are there is bigabego, so big in Bangla means fast, like for those students who understand very fast and who forget very fast.
339000	347000	There is dhirobego, dhiro means slow, like where somebody slow to learn and forgets very fast.
347000	354000	Then there is begodhiro, which is somebody learns very fast and forgets very slowly.
354000	362000	And there is dhirodhiro, like I take a long time to learn, but then I'm very slow to forget.
362000	377000	And he had told me what you want to be in life is the begodhiro, like you want to be able to learn the fastest and remember it the longest.
377000	385000	I found that it's a very good middle class Bengali aspiration, which like most things are not attainable in reality.
385000	399000	So what actually you would want to do, in fact, then that somehow stayed with me, you know, like the earliest perceptions of how humans think and how others perceive others thinking.
399000	409000	From there, it has kind of been that guiding stone that, okay, what if I tweak the system to my benefit, given my apparent lack of capability?
409000	419000	So what would it do, like my ideal combination then became the begabego that I want to be able to learn very fast, forget very fast.
419000	429000	But I want to be able to recall just specific things when I want, because it seems like that's what I'm very used to, like I don't remember what I ate yesterday.
429000	436000	But I have a very good collection of all the things I have cooked and I can make you something which was something like yesterday.
436000	449000	And if can you tune my life to that direction? So just to draw it back to where we have been discussing is sort of this intersectional perspective that there are certain things that human brains are capable of doing.
449000	457000	There are certain things that human collectives are capable of forming and the principles seem to be beyond anthropomorphic.
457000	468000	That is, these are more generic principles. So what are those limitations that we are enforcing upon ourselves by the anthropomorphic lenses that we have put?
468000	479000	Which very interestingly leads us down to the discussion of morality, that what is good, what is bad? Am I a good boy, am I a bad boy?
479000	489000	Now, well, it goes to say good boy, bad boy, good daddy, bad daddy. Yeah, but am I a good boy and a bad daddy?
489000	500000	These are all subjective. So then what's the function of morality? But then, given that both of our, so I know they're up to the Maasala community.
500000	516000	That's where we had met, he was my mentor there. And yeah, I have been stuck like, you know, stronger than it leads to your toes, like kind of absorbing the nutrients.
516000	527000	But then, I mean, from them, I see that as a more of a binary system, like binary star, like two neutron stars, that kind of system.
527000	543000	I was coming to that. Yeah, I mean, so when I say like leech, that was basically the object of morality being applied to give it a connotation of good and bad and parasitic being just a one way communication, let's say.
543000	565000	Which has been something that has constantly clashed because there's probably the only other person I know who speaks the exact same languages, has exposure to similar kind of cultures, and things in that exact nonsensical absurdist generalist, yet specific, but occasionally vague that you won't understand without the concept.
565000	580000	And I, this is all just a long introduction of what is coming up. This is sort of a summary. Somebody hasn't ended. This is still the trailer.
580000	601000	Now, what happens is when you just look into these things, and then look at the world around us, which is, it's like applied philosophy, that you take the basics of metaphysics, and you look at the world around you and try to narrate those things with these lenses, then what does it look like?
601000	618000	And you start seeing the intersections. For example, a lot of this discussion would quickly go into quantum computing for the very fact that it is one of those paradigm shifts, which lets us think differently, which lets us plan differently.
618000	638000	But then a lot of my interest is also to see that what can we do with what we already have? And that is not the same as combining a few tools to build something out of it. It's not assembly.
638000	650000	But it's a careful deconstruction of every tool that we have to see what is it that they're doing? What is it that they're providing us? And when it's a tool, I mean language as a resource. So let's just consider resource, right?
650000	665000	And then putting it back in a certain configuration to see if that is giving us any different results. And this is pretty much how I think both of us think, how we try to talk, how we also try to document.
665000	673000	So I guess I will again now put things back.
673000	678000	So you gave quite a few points for me to jump off.
678000	681000	But now let's build from there.
681000	701000	Yeah. So one of those things that you mentioned about that, the way that is very narrow, very certain, very specific frequency band that we communicate, I've kind of figured out the reason for that. Shukumar Rai.
701000	717000	So I was going to pay tribute to him. He's like a death anniversary. What do you call that thing?
718000	720000	You just called it a death anniversary.
720000	731000	Okay, so that is the term. I'm bad with nouns, essentially, in a sense. So that is 10th September, this centenary, essentially.
731000	753000	And the fun fact is, he would be exactly pretty much 100 years older than me. It gives me a lot of not to think about, so I kind of like working towards that. But point is, I kind of realized that shaped a lot of my childhood, that shaped a lot of my adulthood, and that shaped a lot of the kinds of friends that I am capable of making in the first place.
753000	772000	But it also gave me a very good framework of looking at life, not too chill, not too uptight, but in a particular narrow band where the bandwidth is kind of maximum, in a sense, for the lack of a better way to explain it.
773000	775000	The non-compromising observer.
775000	794000	Yeah, maximally curious non-compromising observer, in a sense. The maximally curious part is kind of important, because sometimes I have to let go of opportunity for the maximal curiosity part, like I want to see more, know more, and sometimes I might need to take less reward because of that.
794000	803000	So anyways, putting that aside, just wanted to mention that because the kind of framework, I didn't know about it even a couple of months back. What was it?
803000	809000	It's like you'd be amazed how much sense absurdist techniques can make.
809000	825000	Ironically. So the other part of that information persistence that you started to talk about, you kind of went into that direction, but you actually didn't traverse. I want to know more about that part. Yeah, it happens all the time. It happens to me as well. I'll do that all the time.
825000	832000	I was like charting the periphery of the quadrant. It's like you take a left from Kronos, don't continue straight.
832000	837000	Okay, so that's the delta quadrant then.
837000	858000	So the parts that we can basically talk about as you kind of lay down, like quantum computing, cognitive psychology, like the entropy of matter, things that we're doing, what is good, the sense of morality in the first place, and how we approach things, what is worth doing.
858000	870000	As in this is one of the things that probably not a lot of philosophical debate that gets into usually like what is worth doing, like what is good, what is bad, that usually is kind of crux of the matter.
870000	879000	What is worth doing? Like after knowing what is good, what is bad? What's your takeaway? Where do you go from there?
879000	887000	So that is probably a very underexplored topic, but there are a lot of things that we can think about.
887000	895000	I think you want to write up. You do better thinking when you write something, if I'm not mistaken.
895000	898000	I do better communication when I write.
898000	899000	Okay.
899000	902000	I do better thinking when I scribble and speak.
902000	906000	Okay, yeah. So for me, that is especially the case.
906000	909000	I do better thinking when I'm vocalizing, subverbalizing something.
909000	926000	I'm pathetic at it, and the reason I stumble, the reason I stammer, the reason I kind of go back and forth and can't find the right word for it is because that is the particular point when I'm actually like threading the needle, or trying to, essentially.
926000	932000	I'm forming my thoughts at that point when I'm actually talking about it, so kind of happening in parallel.
932000	941000	I can collate them better later, but actually the uncharted territory traversing that part, I need to verbalize it.
941000	955000	So I think I am also trying to kind of figure out what are those thoughts, what are those things that we can go through, and how can we kind of make them, give them some sort of permanence.
955000	959000	At least to give our best, to give some kind of permanence, right?
959000	968000	So that's what we are basically trying to probably figure out, and I am trying to make this thing kind of a not a one-time thing.
968000	978000	This is something that we keep on biting every chunk by chunk, and get to the end of it, and come out with a result with some sort.
979000	990000	It doesn't have to be, it doesn't have to generate some magnifying glass or something, but having some kind of goal kind of gives a framework to look up to, essentially.
990000	997000	That's pretty much what I'm coming from. Maybe I need to re-evaluate that at some point, but that's where I'm at.
997000	1007000	I think that itself is a very interesting part to focus on, because in a fundamental way, we are trying to capture chaotic thinking,
1007000	1015000	aligned to people who roughly think in the same domains, but not necessarily the same directions, but with ample intersections.
1015000	1020000	So there is a large union set here.
1020000	1029000	But how does that actually begin to form to a point that actually produces something in the end?
1029000	1035000	And each of these are then loops, or like levels of state, whichever way you plot it.
1035000	1041000	And I think that the important thing is figuring out the loops in the system.
1041000	1048000	So the best way to figure it out is, and again, I'm just depicting what has worked for us in the past,
1048000	1052000	which is we start down a path of inquiry, like go down a rabbit hole.
1052000	1059000	At some point, if the rabbit hole intersects with something you've touched on the path, then you have a loop.
1059000	1065000	So then note that, go back and mark that loop, and then carry on.
1065000	1068000	That don't take that path again.
1068000	1076000	Which is, in our current state, then that is like the point of one article, one library, we don't know.
1076000	1087000	Like each of them then would have traced one full path, which should have some artifact that can be produced as a result of that.
1087000	1090000	Right.
1090000	1097000	One other thing, aside from that, that I was figuring out, that is probably not mutually exclusive,
1097000	1107000	but another way to approach it is, not always two people talking about the same domain, discussing the same thing,
1107000	1112000	have to actually come to a synthesis, not necessarily, because the amount of information available,
1112000	1119000	the amount of discussion necessary, amount of vocabulary available, all those things kind of sometimes may pose the amount of life experience needed.
1119000	1124000	All those things might pose a certain limitation that you might not be able to come to a conclusion,
1124000	1133000	but it might still be necessary to present both the sides of the argument itself as a documentation, in a sense.
1133000	1138000	It is not necessary to come to a, okay, no, we agree with each other now.
1138000	1146000	It is okay to have that, okay, your standpoint and morality stands here, mine is here, but we present both of those things.
1146000	1156000	Maybe we ourselves can revisit it in five years' time, and maybe we have some better tools to deal with that, but it might still be necessary to document it as is.
1156000	1164000	Not mutually exclusive with any of the previous approach we already have tried and worked, but I think that's also yet another thing that we can try.
1164000	1171000	Previously, we had tried to work towards a common endpoint, essentially.
1171000	1178000	So that gives me a very interesting segue into the kind of like the core discussions.
1178000	1186000	Now, a lot of these discussions tend to focus around the abstracts and like the abstraction of the discussion, like the meta, so to say.
1187000	1200000	And kind of like HTTP requests, like if you send reasonable enough information in the header, your body does not need to contain status codes, you know, the kind of APIs I'm talking of.
1200000	1205000	Like, don't send redundant status codes. It's kind of like, don't do small talk, please.
1205000	1209000	I mean, if you just look, if I was an HTTP server, whatever they've said.
1209000	1221000	But then the thing that they've said, in terms of the discussions and then something fruitful coming out of it, and that we record so that we can revisit it in, let's say, five years.
1221000	1229000	Now, one of the themes that also happens is this is the third five-year term that we are entering in that sense.
1229000	1236000	Our first five year was about, let's build open source communities in India in some form.
1236000	1251000	Second five year was on the lines of there are some of these fundamental technologies that we are interested in that looks like we need to do what we actually want to do, which is like things on top of that.
1251000	1260000	And then now at the third five year, where we interestingly have some direction, some vision, and that's there.
1260000	1265000	But then we also have these two five-year stages to inform us from.
1265000	1276000	And there has been a massive increase in the capacity of information processing in this last 10 years or last 11 years.
1276000	1285000	So the way we have communicated, starting from an IRC ping to figure out if this person is alive, exists, or is a bot.
1285000	1289000	And like, was developer a bot was the sort of question.
1289000	1306000	So coming from there to this place where we are at a technological front where this conversation can be recorded, annotated, grouped, archived, cataloged, open for me to search through.
1306000	1312000	Which just makes me think how much more like what I could have done if my class notes were on this.
1312000	1316000	Would I have been a better student? Would I have been able to continue in academia?
1316000	1323000	Because at that point, well, I just had to record the lectures of the professors, come back in my time.
1323000	1329000	Now, before you start thinking that this is me trying to skip classes, no.
1329000	1335000	The environment of the classrooms were such that I could not concentrate in the rooms.
1336000	1339000	I completely understand. You are preaching to the choir here.
1339000	1342000	So I completely understand that particular thing.
1342000	1348000	This is not my best moment of focus, but I have to somehow master it.
1348000	1351000	Or take it, essentially, which is what happened most of the time.
1351000	1353000	So I completely understand. Go ahead.
1353000	1367000	Yeah. So from there, just like taking those ideas and arguments and, you know, at times bundling them into the ferocity of even how we built software on a day to day basis.
1367000	1382000	And sometimes it's not easy to explain to a colleague on why I am taking a certain moralistic stand about even that's an API interface, which seems like this will definitely be damaging in the short term.
1383000	1388000	And I am saying that this will give us a number of benefits in the long term.
1391000	1394000	I think the fundamental reasons have not changed.
1394000	1396000	They have just embellished, sharpened.
1396000	1399000	Now I have maybe a wider vocabulary to communicate.
1399000	1408000	So the next thing to see is what can we do out of the information that we have now in this accessible form?
1408000	1420000	And does this give us the sort of boost that we could have gotten if a lot of those discussions from 10 years were also available in this format?
1420000	1427000	Which, again, is the experiment here at its very subjective level.
1427000	1437000	And that itself opens up pathways to really figure out the core of what does it mean to be in an information society?
1437000	1449000	What does it mean to be interacting with or building systems that are capable of processing information at scales that were not possible here until now?
1449000	1453000	Also knowing that these are exponential growth factors.
1453000	1456000	So then where does that leave us?
1456000	1466000	And if in the process of scratching our own itch, something interesting comes out, then maybe we can repackage that and get rich for a while to do the next thing.
1466000	1477000	Yeah, so I kind of so this is one of the understanding when you say that, okay, previous phases, like if you see that as like one of those...
1477000	1480000	Oh no, I was forcefully bucketing it just to prove a point.
1480000	1484000	That's fine, but I do have a six years phases, essentially.
1484000	1486000	For me, it's actually not five years.
1486000	1489000	It actually does work with the six year phases.
1489000	1493000	So one of the new phases, they're basically starting into 2023.
1493000	1504000	Anyways, the point here is right now what I'm discovering, which is something that I previously put up to the test, like, hey, I'll re-evaluate them next time.
1504000	1512000	It is that we always have access to more amount of data, more wider bandwidth of data.
1512000	1514000	Not talking about the internet data.
1514000	1524000	I'm talking about in general, the amount of information transcended from previous to the next phase or session or like the era or generation, whatever you want to call it.
1524000	1531000	The amount of information that goes posted down is basically more volumetric than previous ones.
1531000	1536000	But you can't take the whole thing away with you.
1536000	1544000	You somehow have to summarize it, turn it into a magazine and utilize that.
1544000	1550000	That's the only thing, because we kind of have a limited amount of compute ourselves.
1550000	1555000	Now that's another part of the discussion, probably later part of the discussion, probably not even today, sometimes.
1555000	1574000	But even if we treat ourselves as individuals that we only have to be individuals and cannot leverage each other's intellect in a positive reinforcement manner, we have to hold the internet model in one GPU, like this thing.
1574000	1582000	Even in that case, we have that limitation where we have to kind of distill it down to some certain life.
1582000	1588000	Might be right, might be wrong, but these are the set of magazines that works for me and I go forward with that.
1588000	1595000	And that came out after hundreds of thousands of years of evolution, essentially, in a sense.
1595000	1600000	Most of it is very recent, but basically it's building in progress.
1600000	1603000	So that means two things.
1603000	1622000	One, even though we will have more compute to analyze and evaluate, like basically, okay, generate all the transcripts from this entire recording, summarize that, categorize that by topic and generate bullet points about what are the things that these idiots have discussed about, right?
1622000	1636000	And that would be very possible, like my later generations, they would not want to sit down and probably have a conversation like this, especially not sit down and listen to a conversation like this the whole way through.
1636000	1643000	They would probably some way figure out a way to distill it down and then do it, right?
1643000	1652000	The point that I'm trying to make here is that it will always be exponentially more and more amount of data that will be available next up.
1652000	1665000	But it doesn't go away from the fact that we still have to somehow pinpoint that this is exactly the center that we are rounding, like going around and around and around, finding out the center.
1665000	1683000	Sometimes when you're pushing the boundary, finding out that nebulous center sometimes is challenging, but it takes a lot of circling to eventually figure out what the center of mass is of this point that I'm trying to make, which is what I'm eventually doing right now, even kind of in a meta way.
1683000	1699000	So, yeah, so we have to figure this part out and whenever that is found, put it up as that maxim that can be carried out, because most of the information that I can study, if it can be art of war or something like that, all of those things,
1699000	1719000	so whenever it comes in a form of like kind of atomic takeaways, for the lack of a better term, that seems to stick around, not only for me, but in general, stick around more than a whole entire novel that probably is trying to make one point.
1719000	1736000	It's probably more interesting to read, but it can get interpreted and misinterpreted, and there are examples of that, like entire third rake was, in a sense, a misinterpretation of Ubermensch in some hot ways.
1736000	1756000	But anyways, the point I'm trying to make is that if you can build right good maxims, in a sense, like very thorough maxims, the number of people moved by it and number of people being aligned, tuned to align this correct direction, probably gets higher.
1756000	1766000	What do you think about that? Or do you think it's necessary to have the whole volume of discussion together with it?
1766000	1783000	I think it kind of leads on to at least two main themes, and I think maxim is one thing I would put a bookmark on for this discussion, because it's kind of one of those inflection points.
1783000	1795000	So let's first tackle the first fork. So we said that one of the things we are also trying to do is figure out what is worth doing.
1795000	1815000	So if I take that direction, like the worthness of something, what adds worth, but let's examine it through the question that what merits archival, like if we look at rephrasing it as what is worth archiving.
1815000	1823000	And that is a problem statement that library sciences has been dealing with since the evolution of libraries.
1823000	1837000	And this is not the same as like where this is different from what clothes do I don't throw away in the sense that at the core of archival is retrieval.
1837000	1857000	Which needs you to index things in a certain way, which means archiving indexing has existed, but information access or let's say unnecessary data or junk data, whichever way you want to put it, is something that has always existed at different scales.
1857000	1877000	The system exhibits different behavior, but if you were to follow the question that what should be archived, that question almost always seems to be a trade-off between how much can you remember or with what accuracy can you remember?
1877000	1899000	Which is where kind of how prophecies, maxims and things like this have existed. Again, prone to interpretation, even if there was a maxim and here by maxim, I mean, let's talk of like one of the things I like to bring in is physics and most of these discussions that just puts at least for me things in perspective.
1899000	1909000	The maxims are like natural laws, you don't create them, you discover them.
1909000	1925000	But which also begs the question that when is it that you can create a natural law? What kind of control would you need on that universe? The very simple layman term is God.
1925000	1949000	But again, what philosophy does is really dissect that concept, right? So if we just approach it like ignore the religious connotation part of it, but just come back to it from the perspective that if natural laws are sacrosanct in the sense that they will always exist, you just merely discover them at some point and after that they become your maxims.
1949000	1964000	So in the same way, then there has to be a journey, there has to be a process or like, well, voyages in like in order to do that discovery.
1964000	1982000	So then where does that take us from, let's say, producing enormous amounts of information today, while also being an amnesiac, increasingly amnesiac society where things are more easily lost than they are produced.
1982000	1994000	So collectively, we might be storing a lot of information at this point we're producing. Well, I don't like to use the word gigantic because they are relative.
1994000	2014000	If the internet spreads across like imagine Starfleet scale, even like the early Starfleet, James T. Kirk, just imagine the amount of data being produced. So what adjective would you then get to? So I'd like to reserve the gigantic things for when things are truly beyond comprehension.
2014000	2037000	But in any case, the case at point is that we have an acute balance to maintain when it comes to archival, that we have to let go of some things and we have to choose to remember certain things, which is what makes it easy to rewrite history by changing your interpretation to suit your current day things.
2037000	2050000	But imagine if history was a mathematically verifiable proof, which it is not today. And again, before historians throw daggers at me, I'm not saying that there is a technology solution to everything.
2050000	2077000	But if, if history was a mathematically verifiable proof for which you did not need to see the event to happen, like event happening for you to know that it's a proven thing, then we kind of reach the state where you don't necessarily need to store all of history, right?
2078000	2080000	You don't need to remember all of history.
2081000	2084000	And I'll open the field there.
2084000	2098000	Yeah, yeah, you poke me in a particular point, where if you look at the world event, I'm talking about history from a physics perspective, the interaction between bodies.
2098000	2112000	Yes, right. If you look at it, and if you can look at any frame of a certain event, not only you can predict where it is coming from, it can you can also predict where it is going.
2113000	2118000	And that's the premise of psychohistory, Harry Seldin in Foundation.
2118000	2135000	Yeah, so which kind of points towards that if you actually knew world history of at any point in time to a certain level of detail, you can predict the going forward part and going backward.
2135000	2143000	I'm talking about an ideal situation, not necessarily a humanly comprehensible model, at least humans of 2023.
2144000	2149000	But the point is, that is actually possible, in a sense, mathematically that is kind of possible.
2150000	2166000	The part where we come short here, which is again one of like quantum computing breaking cryptography, the way traditional computing said that this cryptography cannot be broken, so 256 bit is good enough for it.
2167000	2175000	And there comes quantum computing that says 256 bit doesn't matter. Yeah, I don't give a shit about this.
2175000	2180000	So point is, it breaks the assumption, ground level assumption.
2181000	2199000	If we are talking about why are these new language models and new basically the all the entire generative AI is doing so well is because the amount of memory that is we can allocate to do some certain things is humongous.
2199000	2219000	The point we can extrapolate each pixel to 1536 dimensions, which is which was not something that is the which is not something that humanly possible consciously at least apart unless you are some kind of savant on that particular thing, or it was not possible with any traditional hardware.
2219000	2224000	So things that we thought are impossible are being very possible right now.
2225000	2239000	Similarly, the exact quantified history about knowing a certain event and then being able to predict the past and the future of it, it is going to be possible very, very much possible.
2239000	2245000	It already kind of is if you think about weather prediction, how that works, it already is in effect.
2245000	2249000	We are just not using it for that particular situation or purpose.
2249000	2259000	But I don't see any reason why if we follow along the journey that we are already headed in the exponential growth of compute, I don't see why that cannot be true.
2260000	2264000	So I kind of got poked in that particular rib.
2264000	2266000	So that's why I kind of like expanded on that.
2266000	2271000	But I think you had more points to be made there in case I like cut you off.
2271000	2272000	Please go ahead.
2272000	2278000	No, no, this is this is in that direction only because see what I was trying to say is
2278000	2290000	that there are certain questions around remembrance around like public memory or let's say collective human memory.
2290000	2295000	And one of the things I have written time and again is that collective human memory spontaneous.
2295000	2302000	And it doesn't mean it is it doesn't exist.
2302000	2306000	It also means that it is that sporadic.
2306000	2311000	And that's again that's the nature of things.
2311000	2320000	If you break it down, you would eventually find a chaotic system at work with again entropy going towards like tending towards high over time.
2320000	2324000	So if you again break down everything fundamental enough.
2324000	2332000	So then if you then think of why I brought up what is where I'm trying to get my punchline together.
2332000	2336000	So what merits archival?
2336000	2345000	The decision is kind of based on what preserves the lower entropy state.
2345000	2355000	It's a constant grappling of a higher entropy entity or a higher entropy state to remember a lower entropy state.
2355000	2366000	And which is your essentially if you then look at entropy as that metaphorical river, you are going upstream, which is remembering the past gets exponentially difficult.
2366000	2371000	It's not a linear trajectory.
2371000	2373000	And that is simply if you again.
2373000	2374000	Yeah, good.
2374000	2380000	I think probably that points towards the intent of it.
2380000	2381000	Yes.
2381000	2393000	And I was just going to say that there are certain things like on the same point, certain things that are possible now, which would you would want to then look at the same questions in a different way.
2401000	2407000	Sorry.
2407000	2408000	Can you hear me?
2408000	2410000	Yes, I can hear you.
2410000	2413000	What happened?
2413000	2416000	No, it's my headset just keeps on disconnecting from time to time.
2416000	2418000	Okay.
2418000	2419000	Am I audible?
2419000	2422000	I might be a little bit low on this audio scale.
2422000	2424000	So sorry about that.
2424000	2428000	I'm just now I was trying to figure out if I'm hearing you from the external speaker on headphones.
2428000	2429000	Yes.
2429000	2437000	So no matter how much you improve technologies, certain things never change like this.
2437000	2442000	There are different, different variants and you can keep on going throughout the ages.
2442000	2445000	Oh, I did not see your fire sign across the hill last night.
2445000	2446000	I'm so sorry.
2446000	2451000	Can you see my fire sign?
2451000	2453000	I mean, just the point.
2453000	2454000	Yeah.
2454000	2455000	Sorry.
2455000	2456000	Yeah.
2456000	2457000	Little detour.
2457000	2460000	You remember the penguins of Madagascar that movie where?
2460000	2461000	Yeah, yeah.
2461000	2465000	Dave and he is trying to.
2465000	2466000	Dave?
2466000	2467000	Yeah.
2467000	2469000	Dave.
2469000	2471000	Anyways, anyways.
2471000	2472000	Same thing like that.
2472000	2481000	That seemed that hit kind of like in a particular court for everybody who ever has been into meetings of this kind.
2481000	2496000	And you saw somebody where this hoodie encoded last day, which wrote you were on mute.
2496000	2497000	Okay.
2497000	2500000	Popular culture catching up.
2500000	2512000	So anyway, that is one of the afterburner effects of how probably the two of us here look at what is worth doing.
2512000	2521000	Turns out to be a very misanthropic concept at its outset, but probably, well, not as much misanthropic.
2521000	2528000	It's selectively anthropic, let's call it.
2528000	2531000	Yeah, I have a question there on that.
2531000	2534000	What is anthropic?
2534000	2535000	Hmm.
2535000	2539000	I try to dissect Latin a bit.
2539000	2543000	And my knowledge of, again, the misanthropy is vague at this point.
2543000	2545000	I'd rather not comment at this thing.
2545000	2557000	Let's just look at it as the.
2557000	2569000	What is it when the compatibility that it's a misanthropy is a non compatible component in a society of humans.
2569000	2570000	Oh, no.
2570000	2571000	Somebody.
2571000	2574000	So you're already assuming you're already assuming.
2574000	2575000	Yeah.
2575000	2576000	Anthropic system beforehand.
2576000	2577000	And you're trying to.
2577000	2578000	Yes.
2578000	2579000	No, that was.
2579000	2580000	Yeah.
2580000	2581000	My question was.
2581000	2582000	I was just trying to show the loop.
2582000	2583000	Okay.
2583000	2584000	But yeah.
2584000	2597000	So the question, if you try and understanding what is humanity, what is anthropic, the more you try to see, look into that, the more it turns out to be.
2597000	2614000	What ever, what are the traits we term as humanitarian are exactly the opposite of what actually pushed us to become the humans, the dominant species.
2614000	2622000	That the last straw of that is human versus Neanderthals.
2623000	2624000	The last straw.
2624000	2642000	The last provable bit is that Neanderthals, they weren't like they had the exact and like similar kind of social bonding, the empathy and like caring for others.
2642000	2649000	All the other all the traits that we say these are the good like working with as teams, etc, etc.
2649000	2654000	All those things that we talk about as these are the good traits to have.
2654000	2656000	And if you say that that's what made us humans.
2656000	2658000	No, they also had that.
2658000	2668000	What did they not have is that Machiavellianism for a lack of a better umbrella term.
2668000	2678000	But you mean the prince, like not having an organized direction to channel the collective energy.
2679000	2690000	In a sense that basically attacking the enemy at night, knowing that they already don't want to fight at night.
2690000	2701000	It is not moral or it is not acceptable, ethical to fight at night and knowing that using that as an information to fight at night.
2701000	2705000	What did you call that?
2705000	2713000	So that those are the kind of things that made humans humans.
2713000	2718000	Like I'm probably just showing the tip of the iceberg.
2718000	2729000	You can keep on going down that rabbit hole and keep on trying to find out more and more personality or psychological traits that are absolutely heinous.
2729000	2732000	And that still lingers on, by the way.
2733000	2741000	But those are the ones kind of gave us the edge over the other ones who had not much differently.
2741000	2754000	Even if you go down from hominid to the pan, basically the chimpanzees and gorillas, they also have the social structure in the same way.
2754000	2759000	They also show that their child dies. They show the same kind of sympathy.
2759000	2765000	But what about anthropic humanity that makes us human?
2765000	2769000	If you keep asking that question, it kind of gives us a very dark answer.
2769000	2783000	Exactly the things that we grow up learning as the traits to have as humans are not the ones that made us if we actually stuck to only those ones.
2783000	2791000	I'm not saying that those are not important, but I'm saying that only pure silicon does not create a good microchip.
2791000	2801000	You need the doping in order to create that semiconductor that actually can be turned on or off.
2801000	2810000	That doping, that dopant part is the one that is not really the light part is the dark part.
2810000	2816000	So the moment you talk about that, a kind of like it might sound misanthropic.
2816000	2824000	Most of the utilitarian or things that generate result drive result are misanthropic.
2825000	2843000	If you psychoanalyze most of the CEOs of Fortune 500, you're not going to find the answers that you would like that these are the best human beings that driving the society or culture or at least from a textbook perspective.
2843000	2851000	You have studied this. You can talk more about that part. What do you think from that perspective?
2851000	2858000	I think that's an eventuality of any species that does not have a self sufficient energy system.
2858000	2873000	And because as you as you break that question down and you're going to first encounter the stage of ethics very soon,
2873000	2884000	because this is fundamentally an ethical question with a political philosophy very strongly ingrained in it.
2884000	2897000	Because a lot of these thought systems have also evolved in order to just look back and identify what the hell had happened there.
2897000	2906000	And in the process, like what what was it that happened and then building those systems to come up to a point that we could explain all of this.
2906000	2915000	And then, you know, as happens with intelligent brains that once you are looking at it for too long, you start seeing variations in it.
2915000	2919000	And well, that is how pretty much all branches of philosophy evolve.
2919000	2930000	You cannot have a well informed argument in philosophy if you have not looked at that thing long enough for you to see the details in it.
2930000	2942000	So for from that perspective, when you start breaking down the concept of humanity at the core of it would be the conquest for resources to survive.
2943000	2952000	One survive, then thrive, one thrive, then conquer, one conquer, then build, one build, then colonize and so on and so on.
2952000	2956000	Or maybe even in the in some loops thereafter.
2956000	2974000	But then the thing then what we cannot change here, unless we are a self reliant, like we have a self-sufficient energy system.
2974000	2985000	When I say energy system, just imagine the combination of an unlimited uterium supply and unlimited replicators in the Star Trek context.
2985000	2993000	If you have those two technologies or those two resources available in vast quantities that can change any society in any direction.
2993000	2999000	At that point, direction is a matter of the forces immediately pushing it till the resources expire.
2999000	3006000	Now imagine in a universe where you cannot destroy replicators and dilithium resources.
3006000	3011000	Just imagine a game world like they are non-destructible resources.
3011000	3013000	You can go bang against them, nothing will happen.
3013000	3025000	So eventually the entities in that world will learn to live with the fact that they can build anything at any point and with unlimited energy.
3025000	3028000	And suddenly the craze for resources do not exist anymore.
3028000	3034000	And we have reached that platonic ideal state of sorts where you don't need to have that lust for hunger.
3034000	3039000	And you can then focus on something that is a higher principle.
3040000	3049000	The disconnect between all, let's say the key drivers of human driven change, let's say.
3049000	3057000	And I mean the people ultimately responsible for whatever actions humans are performing at mass scales.
3057000	3066000	Those people usually portray this imagery that they are already at that stage where they have a self-reliant system
3066000	3070000	and that they are just doing everything for the greater good.
3070000	3074000	All I'm saying is that is necessary. It will eventually be part of the system.
3074000	3087000	But for now, the thing that we need to remember is that we are operating under limited resource in a world where we are not energy self-sufficient.
3087000	3092000	That is, we need to extract energy from other places.
3092000	3102000	With that in context, then the question of ethics shifts from is doing that good or bad to how can you do that better?
3102000	3110000	That we are today extracting a massive amount of meat from animals.
3110000	3116000	So you can use the same tool now to dissect any concept and go into that debate.
3116000	3130000	But what I'm trying to say is I think the question that is that good or bad, like is progress good or bad, eventually is a polarizing question.
3130000	3146000	And it's polarizing because, well, too much progress and not enough progress have only one meaning at the scale of the universe.
3146000	3153000	And that is maximum entropy saturation and no entropy at the beginning of the universe.
3154000	3162000	Between that, between if you need to, I mean, if let's say we for the sake of the discussion that we must make it bipolar,
3162000	3172000	let's make sure the poles are that far apart and be humble in approaching what we are approaching in that space.
3172000	3182000	So then anthropomorphic or anthropogenic concepts become, that is what I was trying to say, they become a limiter.
3182000	3190000	And you can again go into the argument for good or bad reasons, in which case see the last 10 minutes of this talk.
3190000	3199000	But what I'm trying to say is that we can instead look at the fact that there are certain things we cannot avoid.
3199000	3208000	Like I cannot keep my cat from having a good time outside just because another cat outside is terrorizing this cat.
3208000	3214000	If I keep the cat indoors, I will keep it the safest, no doubt.
3214000	3226000	And if you want to make that into a question of morality, then is that the right use of those few CPU cycles?
3226000	3228000	That is a question of morality.
3228000	3237000	Yeah, I know. So I get where you're coming from. I kind of have follow ups. Sorry, I might be too not loud, essentially.
3237000	3241000	What is opposite of loud? Anyways, I can hear you clear.
3241000	3253000	So the world of abundance, energy self-sufficiency, I'm not using that term because I'm just using a more generic way that makes sense to me.
3253000	3262000	So just to be clear here, when I said energy self-sufficiency, I meant in the true sense of us having a cyclical energy economy.
3262000	3271000	You're technically correct, but I'm saying world of abundance because before we reach that energy self-sufficiency,
3271000	3281000	the reason it's tail chasing perpetual is because you can keep on going.
3281000	3291000	The more amount of energy you chase, the more amount of energy you need to chase that, it kind of gets to a point like how many Kardashev skills do you want to go?
3291000	3295000	So that's the reason I'm not using that. I'm just using a more generic term.
3295000	3302000	Let me just interject here once. What I was trying to really make the point is that these are important questions, but these are not urgent ones.
3302000	3309000	These are questions that we need to consider when we are at that stage without which we will constantly be in this phase.
3309000	3316000	And there are more immediate concerns and there are more immediate possibilities that we would get to.
3316000	3323000	And just to sum that up, one of the areas that I was talking of or trying to get to is that when we are looking at morality,
3323000	3328000	or let's say, should I do some like that archival context, right?
3328000	3339000	One of the things that we can do with a lot of the systems that we have right now is again following Floridi's description that we run more simulations.
3339000	3345000	That one of the things if you, so this is where I want to tie that to the maxim, but that I am not against maxims.
3345000	3351000	I understand the value of maxims. I am terribly skeptical of when maxims are set in stone.
3351000	3361000	I am very much fond of maxims that are the output of an elaborate computation which gives me a certain predicted result.
3361000	3370000	And similarly, if the input conditions change, the output might change, but the output will still be provable based on the input conditions that I have received.
3370000	3377000	So if those are the maxims, the issue we have with the maxims of till date technology is that maxims had to be written in stone
3377000	3387000	because they had to convey the largest possible information and they were open to understand, open to interpretations.
3387000	3395000	Now, what if you don't just save the maxim? You save the version control history, for example, that led to the creation of the maxim
3395000	3402000	so that lawyers of future date, when they use this maxim as a founding principle, they can also analyze the maxim if they need to.
3402000	3412000	And that's just one aspect that what we have gained is the ability to simulate a lot of this, store a lot of the state to make it replayable,
3412000	3420000	which brings back to that verifiable history as an archival thing.
3421000	3440000	I agree on that part. The part that I am seeing that more can be added here is that when you are saying that, OK, which part do we need to decide to keep?
3440000	3447000	I'm seeing the pattern that we actually don't need to because we don't know which part actually we end up being important.
3447000	3460000	Yeah, but what we can do, so then we don't need to, it does not mean that the system is that black ship of in the restaurant at the end of the universe.
3460000	3468000	The restaurant at the end of the universe, where it is just one voice command control and I'm not talking of that.
3468000	3478000	I'm talking of intervention under the right layers, which is kind of let the mass scale system, the mass data processor do its work
3478000	3488000	because it's already been trained or you can let's say tweak the parameters, whatever, but let it do its work, step back, but have the capability of chiming in.
3488000	3502000	For example, keywords, for example, just the ability to indicate that, OK, go back and switch from here, but which are all in essence better retrieval.
3502000	3514000	Yeah, so think about it in this way. The way mummies are kept, they were not creating that as a time capsule so that the latter point in time people can dig them up
3514000	3522000	and learn more about their history, which kind of oils were being used, what is the isotope and all. They didn't know that they didn't have to find that out.
3522000	3532000	They didn't know that, OK, the fingerprints that they leave are going to be uniquely identified that how many people actually worked on this particular month, those kind of things.
3532000	3543000	I have a very interesting what I had once written on this. Let me just see if it fits here that one of the answers to what is worth archiving.
3543000	3553000	I mean, I had asked you a question I had, of course, thought through a bit is that a very different thing that I want to live in a situation like let's imagine a hypothetical society
3553000	3573000	where as we live our daily lives, libraries form, as generations pass, museums form, as more people want to play parks form and as more people just want to stay quiet, things don't change.
3573000	3586000	Yeah, and so that is kind of. Yeah, that's profound. That is one of the maxims in a way. If you can just pack it together in a good package, that is the maximum.
3586000	3592000	I understand that you don't have to explain, right? No, I'm just explaining for the sake of the technicality.
3593000	3606000	This is what I mean when I say that the answer to what should be archived should not be a specific answer, which is if you're writing software, do not give me specific if then else, please use functional programming.
3606000	3615000	Please think of data transformations, data flowing through a system and write functions to transform at those intersections rather than dealing with discrete data.
3615000	3629000	And similarly, when we're talking philosophy, we very quickly go from a more abstract concept to a discrete concept and still think it is abstract, which is where the fallacy is creeping.
3629000	3640000	Those are the places where I get concerned if I am not caught, like that's when I would expect somebody like you to hold my feet to the fire, essentially.
3640000	3645000	If I ever fall to those, I don't don't try to jump around the as in dance around the topic.
3645000	3649000	If ever I am making any kind of like a fallacy.
3649000	3658000	The only time I dance around the topic is if there is a force field around it that I can't get through to it.
3658000	3667000	Or let's say even better, a bonfire in the center of it, a force field around it, and I am really in a good mood to run around the fire.
3667000	3669000	Maybe there's also a good music.
3669000	3673000	Yes, in which case, maybe that's the topic. But let's say it's like some sort of it.
3673000	3681000	Yeah, so the what I was trying to say, I forgot. Okay, so the intent part of it, so some part of it.
3681000	3686000	So the reason when I asked, right, I also kind of thought about that part.
3687000	3708000	When I say that what is worth doing after deciding what is good to do, the intent eventually pushes itself in like nature, pretty much in all of this discussion is in like deciding who is who is deciding what to do.
3708000	3722000	There will basically forming the next part of the fabric, essentially the pattern in the fabric of reality, if they choose to go forward and pursue that essentially.
3722000	3740000	So at the center of it, there is nobody telling you that you need to do this. It's you deciding to tell yourself that I need to do this.
3740000	3747000	Unless your managers put it on the sprint roadmap without consulting you.
3747000	3756000	I mean, I get the joke, but my counter joke, which will not sound as a joke is that you have to be able to drop the manager.
3756000	3762000	Before you drop the manager, can you assume to be the manager?
3762000	3772000	No, no. If you kill the god, you now have the responsibility to be the god. I'm coming from that perspective, but I know that.
3772000	3788000	Fair, fair, fair. Again, what we were just about to take what two different moralistic stances and based on the initial variable, like that very input into that function, the output would have been different.
3788000	3798000	And these are the places where I said that when your discussion comes full circle and you see a fork in the road back again, but it looks like one lane you have already come through.
3798000	3807000	That's when it's time to mark that put a direction that this was already covered. Now we go here so that you can go in that direction.
3807000	3811000	And how do we mark this one?
3811000	3818000	Well, let's market with.
3818000	3826000	I don't know. I'm very curious to see what the system does. I don't want to give too many inputs at this point.
3826000	3835000	I just want to see what comes out because it's like, you know, you've bought a stock car before you write it and understand don't tune it.
3835000	3840000	Because you won't know what exactly changed as a result of the tuning. So like this is my first time.
3840000	3846000	So recently I come like I came across that would be the right time.
3846000	3852000	So a particular situation where this is this made a whole lot more sense.
3852000	3870000	So the GRR Tolkien's interpretation of where he was coming at, like the moralistic, his moral standpoint, why he was against Sauron or technological development, in a sense, which Sauron embodied.
3870000	3878000	The issue with any Tolkien modalities, it can be dethroned in one shot, but yeah, take it in the context of what you're saying.
3878000	3892000	But yeah, so what Sauron represented is that knowing that Eru Il-Hutur exists, he kind of like, no, I am going to replace you with myself in a sense.
3892000	3905000	Like I'm talking about the new I have I don't know whether you have watched it, the Prime series, the Rings of Power.
3905000	3907000	I don't think you have watched.
3907000	3914000	I don't know if I have not. If I started then I started then it had some place where yeah.
3914000	3924000	Yeah, you probably wouldn't like it as somebody who has invested interest into the cohesiveness of the world that you have already known.
3924000	3934000	And though from the light from the perspective from which light you have seen it, it would seem like a kind of desecration in a sense.
3934000	3938000	Like somebody is one screaming before you continue.
3938000	3955000	One thing to note about how I potentially look at a lot of the rings is it's an revisionist history of a massive, massive peasant uprising that kingdoms were forced to unite and bring down.
3955000	3959000	And when you put that lens, then everything else makes that much more sense.
3959000	3965000	Then why Aragon is being this douchebag Aragon makes more sense.
3965000	3974000	And why why like repair this one blade over the cost of this fucking get a new one made man might even get you better technology nowadays.
3974000	3979000	But no, the emblem was more important than the weapon itself.
3980000	3982000	Where things that just make it.
3982000	3987000	I'm just saying that, of course, I have read it and I understand it as Tolkien intended.
3987000	4004000	But I also see it as a potentially in looking at our current times that it kind of fits almost as if it's a revisionist history that there was some other history which we if we dig deeper in some lost scrolls like the charwaks will find something else existed.
4004000	4011000	When you when you say that it's a revisionist history, it tells me everything that I need to know about your standpoint on that.
4011000	4019000	I don't need more explanation than this, honestly, because I don't know what is relevant for archival.
4019000	4023000	No, so I'm also marking my own bookmarks for future retrieval.
4023000	4032000	Right. So knowing that the way I've seen most of the Tolkien fans discuss this or come come at it from no, no, no.
4032000	4036000	You are basically this is sacri, what do you call it?
4036000	4049000	Sacrimonious for the most people they think the most feminist moment in Lord of the Rings was you opening her helmet and saying I'm not a man.
4049000	4053000	Yay for feminism. That's like their the peak of their understanding.
4053000	4057000	So yeah, so so that's sanctimonious position.
4057000	4059000	If you hold it, then it would hurt you.
4059000	4066000	But if you can open your mind like, OK, this is a world that can have multiple different interpretation in different people's eyes.
4066000	4070000	How can you say this in this world?
4070000	4077000	Galadriel is not so much so different than the enemy that she is trying to kill.
4077000	4079000	And it is very clearly shown.
4079000	4088000	Yeah, and eventually she discovers that she is pretty much word for word the same entity.
4088000	4092000	And if she does win, like you can take sides, you can pick sides.
4092000	4105000	But if and the person that Sauron is trying to like rage against, like fight against or like replace cannot like basically in the story's frame, you cannot.
4105000	4112000	The person who created the universe, but trying to the directionality is on the same way.
4112000	4118000	Like I will enforce my will onto this world. Innocence.
4118000	4125000	End of the day, when I replace you, I will rewrite the history the way I see it fit.
4125000	4129000	And people will know you were the bad guy. I am the good guy.
4129000	4136000	So whatever is shy clue in the before is the Satan after essentially innocence.
4136000	4146000	So if you see it, see that from that perspective, you also see that Galadriel is effectively the same person, not so much different from Sauron.
4146000	4151000	And if she ends up winning, she will not do something that's different than her.
4151000	4159000	She's likely to dismantle the patriarchal power structures that exist in the world.
4159000	4167000	If that is what you want, that is your intent to preserve that history.
4167000	4175000	And you are more into no, I am more into I am into this patriarchal structure or I don't care about that.
4175000	4180000	I just care about their technology and how much they are building, et cetera, et cetera.
4180000	4186000	And you choose to be on the side of Sauron. That is also an option. You can do that.
4186000	4194000	And end of the day, whoever ends up winning will end up writing that history turns out in this particular story, Sauron ends up losing.
4194000	4203000	So his people are uglier and shorter and speak a language that the book is not written in innocence.
4203000	4208000	It's a dark language. He ends up winning.
4208000	4213000	That is the language of the book. And the other language is the dark language.
4213000	4220000	The point I was trying to make is that all those discussion about morality, it kind of comes down to that.
4220000	4235000	What is the person who is observing the person who is narrating and the person who is intending to preserve the part of the history, which side they want to take effectively comes down boils down to that.
4235000	4240000	So at the end of the day, you kind of have to take that.
4240000	4248000	I want to preserve this part of the history. I want to make this part this kind of changes, this change in entropy.
4248000	4253000	Oh, I have another example on this. So entropy and hold on to that.
4253000	4263000	And if you can end up executing it, now the world has that scratch mark, that entropy imprinted on its reality.
4263000	4269000	Think about how do you create a stone statue?
4269000	4273000	Think about the entropy of a stone statue.
4273000	4280000	We say that entropy is something that is basically disintegrating and taking a low energy state.
4280000	4288000	No, but that is the misconception of understanding of entropy. Entropy is not disintegrating.
4288000	4301000	How would you define because for a statue, somehow some sense had to come together or you're muted.
4301000	4303000	You are muted.
4303000	4305000	I am not on mute anymore, but there is a lag.
4305000	4308000	OK, sorry, go ahead.
4308000	4315000	Yeah, so I was saying just pointing out the fact that it's not entropy that is decreasing.
4315000	4323000	It's the action like the entropy as the agent of change, because entropy is growing in that sense.
4323000	4328000	So just because in colloquial language, we kind of keep on doing this switch.
4328000	4337000	We cannot. So the point I'm trying to make is that you can't and this is not the known.
4337000	4342000	So this is you can call it original research or only the original thesis or something like that.
4342000	4348000	This is not science does not approve of this yet.
4348000	4350000	This is not peer reviewed.
4351000	4364000	But I'm positing this is that you can't say which one is the direction of entropy by being inside of a system unless.
4364000	4374000	So for the designer, the entropy would always seem to go towards the end state that is more organized.
4374000	4384000	But for the players inside the system, it will always seem to go towards a more disorganized, even if the events are exactly the same.
4384000	4388000	Well, you can prove the statement with simple linear algebra.
4388000	4399000	I mean, I'm just saying that this is one of those very axiomatic statements because you need to be able to let's see, even if it's a matrix,
4399000	4409000	you need to be able to write the matrix in a way that you have at least one more dimension for you to compute the values of that dimension.
4409000	4422000	So you cannot by any chance know what is going to come out of a system unless you have access to a higher dimensionality from which to observe and compute the current dimensionality,
4422000	4429000	which also makes the capability of dimensionality increase and computing as unidirectional.
4429000	4435000	You can only observe higher from higher dimension or lower dimension cannot do the other way around.
4435000	4441000	So these are in the same sense, the established laws in physics, which again apply right here.
4441000	4450000	Yeah, this is one of the topics I just reminded me that we are probably looking back in that we need a higher dimensional to in order to comprehend the lower dimension.
4450000	4454000	This is the deja vu point. OK, so that part we understand now.
4454000	4459000	So with that framework in mind, think about the person.
4459000	4465000	If I'm a sculptor, for me, it's a block of stone.
4465000	4468000	I am chipping the stone away.
4468000	4471000	That is not the statue.
4471000	4478000	And what remains is the statue and a lot of broken stone chips everywhere else.
4478000	4486000	If you are somebody who doesn't, it's basically a fish that doesn't understand the concept of statue.
4486000	4495000	Looking at this entire event, sees that there was this big stone and now there's a lot of chips and one big chip.
4495000	4499000	This is how entropy, this is the direction of the entropy.
4499000	4505000	It is basically disintegrating. It is for breaking down. It is like basically flattening everything.
4505000	4511000	That is where it is seen. It doesn't see the intent was to make a statue.
4511000	4513000	Is it that statue?
4513000	4520000	If that is the case, that stone that is crumbling around, that is basically a byproduct.
4520000	4527000	Either you dust it away or you basically move on from there, take the statue with you.
4527000	4530000	You don't care about that part.
4530000	4533000	If you are the designer, it came to life.
4533000	4537000	You actually reduced entropy in a sense.
4537000	4542000	End of the day your end result was supposed to be the statue and now that is.
4542000	4551000	Whereas what started with this journey with the stone started with a lot of silica dust.
4551000	4557000	Eventually probably formed under some ocean basement millions of years back.
4557000	4560000	And then gradually got pushed out.
4560000	4564000	So if you think of that whole thing as a time lapse, what happened?
4564000	4570000	A lot of random particles came together, formed something and then became a statue.
4570000	4573000	And a couple of them stayed there.
4573000	4579000	Does it not look like a completely opposite of what we say that things cannot happen?
4579000	4581000	Entropy doesn't work that way.
4581000	4586000	Things don't come together in a shape. They just fall out of that shape.
4586000	4590000	So you can completely show that that is actually possible.
4590000	4592000	That's something that you can do.
4592000	4602000	Now that is only the case when the creator or designer, somebody with an intent is enforcing that.
4602000	4607000	That's only the time when seems to be entropy is actually going backwards.
4607000	4614000	But for the players, for the NPCs, it will always seem to go to a higher entropy state.
4614000	4619000	Even though the events are exactly the same.
4619000	4624000	Yep. So I mean, entropy is like the ultimate Borg.
4624000	4628000	The intentions are irrelevant. You will be assimilated.
4628000	4636000	Yeah. So the reason for me to bring this one out is kind of to put a pin on that entire discussion about.
4636000	4651000	So we probably are not done with the discussion, but probably still to put a pin on what is worth salvaging or like storing or basically collecting or documenting and keeping for a long term storage.
4651000	4658000	There's a lot of it is basically intent towards something.
4658000	4669000	And the ability. So then basically to touch along with that is a tamper proof history.
4669000	4678000	As a way of ensuring lesser degradation of the knowledge over time.
4678000	4689000	If you and then again, tamper proof history will also, of course, be it will only be from a certain point in time forwards till you have higher dimensionality.
4689000	4698000	Right. But just to wrap up. So the tamper proof history is important in the sense that you're not storing all of the information.
4698000	4701000	The tamper proof like it's basically zero knowledge proofs.
4701000	4710000	It's smart contracts equivalent of things that you are storing, not just the maxims, not just the learnings or the archives or the intent.
4710000	4726000	In this case, if you want to guarantee that it won't be tampered with so high guarantee versus a loose guarantee, the high guarantee should also have mechanism for you to reproduce that final state like a theorem.
4726000	4735000	So to say, then what you archive, what is worth archiving, primary answer would be the intent.
4735000	4742000	Because it's only when you apply intent frame on frame on frame, intent action, intent action, intent action.
4742000	4754000	If you believe in that homogeneous causality, that is, if you want to implement a homogeneous causality, then let's just say in kit terms.
4754000	4768000	Continuous. Yeah, a continuous, but unidirectional causality of equal intervals like intent precedes an action.
4768000	4778000	So the in this case is simply going to be a high guarantee.
4778000	4788000	If you are storing only the intent, sorry, low guarantee for storing only the intent, a high guarantee if you're storing the intent along with the replay mechanism.
4788000	4796000	But just because when we say low loose guarantee does not mean falsification.
4796000	4806000	Lose guarantee is simply saying that you will have to take this as face value because you don't have any way to reproduce this to know for yourself.
4806000	4809000	But it has not been changed.
4809000	4813000	OK, that is a good as in a baseline.
4813000	4820000	I have two things to add there like nothing to like it's not doesn't exclude from what you already said.
4820000	4826000	I want to add that perspective, adding more perspective to existing data.
4826000	4831000	So basically multiple accounts to the same story to avoid that.
4831000	4836000	What is the term that you used for what history?
4836000	4838000	For a lot of the things.
4838000	4842000	Temper proof revisionist revisionist.
4842000	4862000	So to avoid one single narrative to through some certain filter instead of that coming to become like being treated as a history, let all the different filters play their role and record all of those.
4862000	4868000	Don't be the judge and judge decide which one is more relevant, which one is actually correct.
4868000	4878000	So think of it in this way, then your view of history is just a query on that database on the database of history or whatever part of history you have access to.
4878000	4883000	Yes, because there can be many databases which can choose to communicate or not.
4883000	4888000	So at any point you only have a limited view of something.
4888000	4891000	I mean, the best way I can explain it is by Dota.
4891000	4894000	Your map has begun to explode only certain events.
4894000	4897000	Don't be have the audacity to ask for what is there in the dark.
4897000	4898000	Go find it.
4898000	4900000	Yeah, yeah, exactly.
4900000	4912000	So if that is the case, if then, I mean, a lot of the solved problems we are just basically philosophizing game design to a point that why are games designed the way they are designed?
4912000	4914000	There's a very good reason for me to do that.
4914000	4916000	And I will come to that after some point.
4916000	4917000	But yes, you are correct.
4917000	4918000	I know, I know.
4918000	4931000	I'm just again connecting in let's say for the sake of viewers or for us as the future viewer that a lot of what is happening has very strong analogies to the concerns faced by game design and have been answered in different ways.
4931000	4935000	So basically the philosophy for our times.
4935000	4942000	One is possibly the information like the impact of AI, which are again very large scale endeavors.
4942000	4960000	But if you really want to geek out on something more tangible, try to philosophize software or like systems design tradeoffs, the architecture, why something is built, how it is built and why, why, why are games entirely different?
4961000	4973000	Which are also applications themselves of application development such that we distinguish in our software industry as a game developer and an application developer, a JavaScript developer and a game developer.
4973000	4976000	What's going on there?
4976000	4989000	What and what I'm saying is that the things that we are touching has foundations in some of those things for you, extensive experience with Dota for me, certain other things.
4989000	4996000	But you see that some of these also have answers to very complex certain complexities of our daily life.
4996000	5002000	And I'll just at this point just leave two nuggets, which may be pick up at a later point.
5002000	5007000	One of them is the entity component system architecture.
5007000	5010000	What it does, why it does.
5010000	5013000	Can you write it down in the chats?
5013000	5015000	Yeah.
5015000	5017000	Come back to it, essentially.
5017000	5020000	I don't want to.
5020000	5027000	Sorry.
5027000	5030000	Oh, okay.
5030000	5032000	I couldn't hear it properly.
5032000	5035000	Yes, I figured.
5035000	5039000	What we don't mention here is, of course, why it is used.
5039000	5049000	But ECS is one of those architectural patterns that I find very close to my heart.
5049000	5060000	Because it tries to, I mean, it is a sort of, in that sense, superset or an application of functional programming concepts in the way that it is encapsulating the concept of components.
5060000	5069000	But it's kind of a worldview, which is very similar to how I kind of think of the word, like as something that is a state transformer.
5069000	5075000	A massive state transformer with very intricate rules all the way down to the smallest levels.
5075000	5079000	But then entities are independent.
5079000	5086000	But I would just like to use the same things that we're doing to analyze entity component system as a case study.
5086000	5092000	And see what are the trade-offs that it makes, which, again, you cannot get with an object-oriented thing.
5092000	5095000	We can, of course, compare, particularly with a waste of time.
5095000	5113000	And the other aspect that kind of ties this through a lot of this is the edge that, again, like I said in the beginning, why does our existing current AI system, so let's say generative AI or whichever form,
5113000	5126000	I'm just saying status quo and whatever research is in progress at this point, the totality of that, that they seem to be facing certain questions that have been answered again and again.
5126000	5131000	And there are some very smart people who are also thinking along those lines.
5131000	5151000	And then I want to move the discussion beyond just, oh, like, let's say ethics of AI in a generic sense, or should we do this, should we do that, to what are the things that we could not do earlier and hence had not considered at all.
5151000	5155000	And like, and we are able to do that.
5155000	5157000	So I kind of lost the thread there.
5157000	5177000	But yeah, I always think that, yes, when this ties together with the deeply political aspects of our thought and are probably how some of our technology choices are being your mind.
5177000	5183000	Tends to be and again from theoretical perspective that tends to be anarchist in nature.
5183000	5190000	And again, not the bomb throwing anarchist, but the theoretical anarchist principle.
5190000	5192000	So I intend to delve deeper into this.
5192000	5196000	I've been reading up a bit to connect the dots here.
5196000	5202000	And at the same time, it exhibits very strong nihilistic tendencies.
5202000	5204000	And why does it do that?
5204000	5212000	Because a lot of the questions we are asking around archival have also come from the fact that we have lost enough information in our life.
5212000	5215000	It's not coming from an ideological stance alone.
5215000	5224000	It's an ideological stance informed by the losses we have already experienced, which we have not been able to recover, some way or the other.
5224000	5227000	So there's like personal skin in the game.
5227000	5233000	But a lot of these concepts would then look at that one key perspective.
5233000	5240000	So you mentioned some of the things and thanks for like kind of reeling the whole thing together.
5240000	5245000	I know that there are lots of topics that we could go through today itself.
5245000	5252000	But I think it's better that we take the small bite chew over and then come keep coming back and do more of that.
5253000	5259000	Sustaining that high is probably more important than like getting in, being too excited and then burning out.
5259000	5263000	So I think it is better to keep those things out for the future.
5263000	5278000	Couple of things, couple of topics that like one part is that the nihilism, like what kind of energy we are talking about here, establishing those things, going around that part.
5279000	5283000	So I can just add one point here.
5283000	5294000	The idea of the philosophical analysis of some of these seemingly political principles is that they make for very good taste suits for any system that you build,
5294000	5303000	which means I am now talking of that we need to build software systems where our software testers are philosophers
5303000	5308000	so that they can actually know what is the right thing to test the system against.
5308000	5310000	What are my automated tests supposed to guarantee?
5310000	5316000	And I want to approach these from the testability of a software perspective as well.
5316000	5320000	In fact, I would argue that's probably where the world is headed.
5320000	5322000	It's not that that's that should happen.
5322000	5330000	It probably would become very like I think it will just become purely out of several of the fittest in a couple of.
5330000	5331000	I don't really know.
5331000	5333000	I'm very bad at breaking the years.
5333000	5341000	But in two years time, I'm I'm fairly certain that people who have better understanding of epistemic,
5341000	5350000	how to understand the framework itself more than the execution of like basically story point driven output,
5350000	5359000	they're going to start winning out a massive amount to the point that it will become obvious to everybody else that that aspect is way more important.
5359000	5366000	Actual execution of getting the things done can pretty much be delegated to a declarative statement
5366000	5372000	and then waiting for the model to execute that and come back to you.
5372000	5380000	Deciding what is how to tackle that problem, what the declarative statement is, what is worth doing,
5380000	5387000	that part, understanding of epistemic of that part, building the test system where that can be validated or not validated.
5387000	5390000	Those are the parts that are going to start mattering way, way more.
5390000	5401000	And yeah, one of the things that I have seen and I also want to discuss about that is that how was the utilitarian and deontological standpoint
5401000	5406000	and how does that relate to I don't know whether there's an actual background to that,
5406000	5414000	but how does that relate to a declarative versus procedural, not procedural, imperative essentially.
5414000	5420000	I completely forgot. At one point I had told you there are two forks and I went down one fork and I continued.
5420000	5428000	The other fork was supposed to be this declarative versus this discussion as basically the flow-based programming
5428000	5434000	or that's where it came to entity component system as other software paradigms,
5434000	5445000	which if you look at J.J. Palem's papers on flow-based programming between the 60s,
5445000	5453000	they read like science fiction because the computing systems of the days could not even control what were happening.
5453000	5461000	But then if you look at it, that's a system which lets you have logic interposed at places where data is being transformed,
5461000	5465000	which again means that you are then designing a system.
5465000	5472000	So again, if you go to episteme, I found it extremely straightforward when I first read flow-based programming.
5472000	5478000	I got so excited I showed it to three colleagues and they found it absolutely archaic and useless.
5478000	5483000	Then I showed it to a few others and some of them again had this instant click.
5484000	5488000	And that is when I went back to the question that what is the difference here
5488000	5492000	and the difference turned out to be the knowledge of how knowledge forms.
5492000	5496000	So we are talking about the same thing, we are coming to the same conclusion,
5496000	5500000	but we kind of came to the exact same understanding from two different angles.
5500000	5504000	Yes, and that's the whole point to show that that's the intersection.
5504000	5510000	That would be actually, I think that actually requires way more reflection.
5510000	5512000	I don't think we should go into that one.
5512000	5514000	This is just a summary.
5514000	5518000	But this is one of the topics that we should probably cover.
5518000	5521000	The immediate next one probably, we should probably go into this one.
5521000	5523000	Yeah, I think so.
5523000	5525000	This is going to be interesting.
5525000	5531000	And if you want to scope it down, then we can compare the architectural analysis,
5531000	5534000	like I said, of the ECS on those principles.
5534000	5538000	There are some of the things that I have some thoughts that I would like to bounce for.
5538000	5544000	But the other place where I have reasonably thought through for a while is the software testing angle.
5544000	5550000	So I actually want to tackle the question that what does it mean for software testers
5550000	5556000	with the current kind of systems that are coming in,
5556000	5563000	which again, just if you might want to note, the part I want to focus on is the definition of testers.
5563000	5565000	That was the question I was going to ask.
5565000	5567000	Does it have to be software tester?
5567000	5572000	It pretty much seems like any kind of system evaluator would want to.
5572000	5573000	Yes.
5573000	5577000	So the software testing was just to intentionally scope it down.
5577000	5580000	But eventually the question was testability.
5580000	5584000	The ability to test a system being a more important skill
5584000	5589000	than the ability to either build or operate that system.
5590000	5596000	In fact, the second part is basically an extension of being able to,
5596000	5603000	like having the intent, which basically being able to do the declarative,
5603000	5610000	whichever method mechanism you use, being able to declaratively convey the intent
5610000	5614000	and then testing out whether that intent was carried out or not.
5614000	5618000	Those two are the only things basically that is at the crux of everything,
5618000	5620000	like any kind of compute mechanism.
5620000	5625000	And everything else comes down to the, what is the right term for it?
5625000	5628000	Basically to serve these two things.
5628000	5633000	Yes. And we should ensure, we must ensure that interfaces
5633000	5640000	and every other ways of interaction with technologies here on keep these principles in mind.
5640000	5643000	And again, I'm talking beyond software.
5644000	5647000	Yeah. In fact, I want that to be beyond software
5647000	5653000	because it's absolutely fine to use software as the proving ground where we...
5653000	5655000	Simulations.
5655000	5658000	Yeah. But then it needs to transcend that part.
5658000	5663000	And I'm pretty sure I'm more than certain that it actually has a bigger ground to play
5663000	5667000	if we can actually turn it into a model that is cohesive enough.
5667000	5669000	Cool. Fantastic.
5669000	5674000	So I am not available next week and probably the week after.
5674000	5678000	I'm traveling, so we have a work meetup.
5678000	5684000	So I'll come back and let's catch up then. I'll keep you posted.
5684000	5687000	Sure.
5687000	5692000	That was the most depressed you are ever.
5692000	5694000	Yeah.
5694000	5696000	That's fine. That's fine.
5696000	5702000	I probably can try to, but I don't think that would be fair to do.
5702000	5707000	It's not fair to do also because now I have a different time availability factor.
5707000	5711000	So I was like, why don't you do it twice a week?
5711000	5716000	I think that is absolutely a possibility. But once I come back.
5716000	5719000	This is one of the things that I plan.
5719000	5724000	So since 2018, I haven't actually taken a break with my family.
5724000	5728000	My first break since 2018. Oh my God, I got to go rest.
5728000	5731000	Yeah. So basically I haven't really taken a break with my family.
5731000	5734000	And it was not possible because the kid was too small.
5734000	5740000	Now she is just about old enough to know the address and the parent's name
5740000	5742000	in case she kind of goes missing.
5742000	5745000	I kind of think in terms of worst case scenarios.
5745000	5747000	So that's probably the best way to put it.
5747000	5751000	But the point is now it's time we can actually go somewhere.
5751000	5753000	So basically this was a long time coming.
5753000	5755000	So I don't want to take something away from them.
5755000	5757000	So this two weeks is going to be a bit hectic.
5757000	5762000	But after that, when I come back, I'm more than open to like explore this
5762000	5766000	with intensity essentially. Right. Let's go.
5766000	5768000	All right. Thanks a lot.
5768000	5771000	I'm going to end the stream for anybody watching the stream.
5771000	5773000	And then.
